{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f354758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652233ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "zip_path = \"../celebA/celeba/img_align_celeba.zip\"\n",
    "real_10k = \"../celebA/celeba/real_10000\"\n",
    "\n",
    "print(\"Extracting first 10k images from ZIP...\")\n",
    "os.makedirs(real_10k, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    all_files = sorted([f for f in zip_ref.namelist() if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    first_10k = all_files[:10000]\n",
    "    for file in first_10k:\n",
    "        zip_ref.extract(member=file, path=real_10k)\n",
    "\n",
    "print(f\"Extracted {len(first_10k)} images to {real_10k}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5dea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "real_dir = \"../celebA/celeba/real_10000/img_align_celeba\"\n",
    "fake_dirs = {\n",
    "    \"VAE\": \"../vae_outputs/generated\",\n",
    "    \"GAN\": \"../gan_outputs/generated\"\n",
    "}\n",
    "label_map = {0: \"Real\", 1: \"VAE\", 2: \"GAN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b006d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../gan_outputs/generated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Setup\u001b[39;00m\n\u001b[1;32m     25\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     26\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m     27\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     28\u001b[0m ])\n\u001b[0;32m---> 29\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTSNEDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mTSNEDataset.__init__\u001b[0;34m(self, real_dir, fake_dirs, transform, samples_per_class)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m samples_per_class\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, fdir) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fake_dirs\u001b[38;5;241m.\u001b[39mitems(), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     fake_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfdir\u001b[49m\u001b[43m)\u001b[49m)[:samples_per_class]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fdir, img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m fake_imgs]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [idx] \u001b[38;5;241m*\u001b[39m samples_per_class\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../gan_outputs/generated'"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "class TSNEDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dirs, transform, samples_per_class=1000):\n",
    "        self.paths, self.labels = [], []\n",
    "\n",
    "        real_imgs = sorted(os.listdir(real_dir))[:samples_per_class]\n",
    "        self.paths += [os.path.join(real_dir, img) for img in real_imgs]\n",
    "        self.labels += [0] * samples_per_class\n",
    "\n",
    "        for idx, (name, fdir) in enumerate(fake_dirs.items(), start=1):\n",
    "            fake_imgs = sorted(os.listdir(fdir))[:samples_per_class]\n",
    "            self.paths += [os.path.join(fdir, img) for img in fake_imgs]\n",
    "            self.labels += [idx] * samples_per_class\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.labels[idx]\n",
    "\n",
    "# Setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = TSNEDataset(real_dir, fake_dirs, transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e12140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extractor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Feature extraction\n",
    "features, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in tqdm(loader, desc=\"Extracting features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs).cpu().numpy()\n",
    "        features.append(feats)\n",
    "        labels.extend(lbls.numpy())\n",
    "\n",
    "features = np.vstack(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
    "tsne_result = tsne.fit_transform(features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['black', 'red', 'blue']\n",
    "for i in range(3):\n",
    "    idxs = labels == i\n",
    "    plt.scatter(tsne_result[idxs, 0], tsne_result[idxs, 1], s=10, color=colors[i], label=label_map[i], alpha=0.6)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE of Real and Generated Images\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
