{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"test\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3, features_g=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, features_g * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(features_g * 2, features_g, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(features_g, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, features_d=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, features_d, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(features_d, features_d * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_d * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_d * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(features_d * 4, features_d * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_d * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(features_d * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.net(img).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "z_dim = 100\n",
    "lr = 2e-4\n",
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "G = Generator(z_dim).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(n_epochs):\n",
    "    g_loss_epoch = 0.0\n",
    "    d_loss_epoch = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for real_imgs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "\n",
    "        real_labels = torch.ones(batch_size, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, device=device)\n",
    "\n",
    "        output_real = D(real_imgs)\n",
    "        output_fake = D(fake_imgs.detach())\n",
    "        loss_D = criterion(output_real, real_labels) + criterion(output_fake, fake_labels)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        output_fake = D(fake_imgs)\n",
    "        loss_G = criterion(output_fake, real_labels)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        g_loss_epoch += loss_G.item()\n",
    "        d_loss_epoch += loss_D.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Save sample images\n",
    "    with torch.no_grad():\n",
    "        samples = G(fixed_noise)\n",
    "        save_image(samples, f\"gan_outputs/epoch_{epoch+1:03d}.png\", normalize=True)\n",
    "\n",
    "    # Print average losses\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]  Loss_D: {d_loss_epoch/num_batches:.4f}  Loss_G: {g_loss_epoch/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66daa07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"gan_outputs/generated\", exist_ok=True)\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, 10000, 64), desc=\"Generating final images\"):\n",
    "        z = torch.randn(64, z_dim, 1, 1, device=device)\n",
    "        gen_imgs = G(z)\n",
    "        for j in range(gen_imgs.size(0)):\n",
    "            save_image(gen_imgs[j], f\"gan_outputs/generated/{i + j:05d}.png\", normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
