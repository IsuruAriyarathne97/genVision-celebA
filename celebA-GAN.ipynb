{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e2dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:05.615143Z",
     "iopub.status.busy": "2025-05-10T01:11:05.614572Z",
     "iopub.status.idle": "2025-05-10T01:11:11.465259Z",
     "shell.execute_reply": "2025-05-10T01:11:11.464735Z"
    },
    "papermill": {
     "duration": 5.862206,
     "end_time": "2025-05-10T01:11:11.466673",
     "exception": false,
     "start_time": "2025-05-10T01:11:05.604467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95068325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:11.478715Z",
     "iopub.status.busy": "2025-05-10T01:11:11.477837Z",
     "iopub.status.idle": "2025-05-10T01:11:11.594883Z",
     "shell.execute_reply": "2025-05-10T01:11:11.594006Z"
    },
    "papermill": {
     "duration": 0.123681,
     "end_time": "2025-05-10T01:11:11.595925",
     "exception": false,
     "start_time": "2025-05-10T01:11:11.472244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cc257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:11.607651Z",
     "iopub.status.busy": "2025-05-10T01:11:11.606892Z",
     "iopub.status.idle": "2025-05-10T01:11:11.611506Z",
     "shell.execute_reply": "2025-05-10T01:11:11.610753Z"
    },
    "papermill": {
     "duration": 0.011795,
     "end_time": "2025-05-10T01:11:11.612501",
     "exception": false,
     "start_time": "2025-05-10T01:11:11.600706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"gan_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dccd32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:11.624630Z",
     "iopub.status.busy": "2025-05-10T01:11:11.623877Z",
     "iopub.status.idle": "2025-05-10T01:11:11.627339Z",
     "shell.execute_reply": "2025-05-10T01:11:11.626934Z"
    },
    "papermill": {
     "duration": 0.011099,
     "end_time": "2025-05-10T01:11:11.628310",
     "exception": false,
     "start_time": "2025-05-10T01:11:11.617211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "z_dim = 100\n",
    "lr = 2e-4\n",
    "batch_size = 64\n",
    "n_epochs = 50\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d2223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:11.638118Z",
     "iopub.status.busy": "2025-05-10T01:11:11.637772Z",
     "iopub.status.idle": "2025-05-10T01:11:11.641061Z",
     "shell.execute_reply": "2025-05-10T01:11:11.640605Z"
    },
    "papermill": {
     "duration": 0.009716,
     "end_time": "2025-05-10T01:11:11.642027",
     "exception": false,
     "start_time": "2025-05-10T01:11:11.632311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8571ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:11.652567Z",
     "iopub.status.busy": "2025-05-10T01:11:11.652211Z",
     "iopub.status.idle": "2025-05-10T01:11:38.405658Z",
     "shell.execute_reply": "2025-05-10T01:11:38.404579Z"
    },
    "papermill": {
     "duration": 26.760232,
     "end_time": "2025-05-10T01:11:38.407025",
     "exception": false,
     "start_time": "2025-05-10T01:11:11.646793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset and splits\n",
    "full_dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size  # ensure the sum matches exactly\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f7f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:38.418390Z",
     "iopub.status.busy": "2025-05-10T01:11:38.417533Z",
     "iopub.status.idle": "2025-05-10T01:11:38.428468Z",
     "shell.execute_reply": "2025-05-10T01:11:38.427697Z"
    },
    "papermill": {
     "duration": 0.0172,
     "end_time": "2025-05-10T01:11:38.429482",
     "exception": false,
     "start_time": "2025-05-10T01:11:38.412282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3, features_g=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, features_g * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g * 2, features_g, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(features_g, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, features_d=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(img_channels, features_d, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(features_d, features_d * 2, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(features_d * 2, features_d * 4, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(features_d * 4, features_d * 8, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(features_d * 8, 1, 4, 1, 0)),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.net(img).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835698d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:38.439425Z",
     "iopub.status.busy": "2025-05-10T01:11:38.438680Z",
     "iopub.status.idle": "2025-05-10T01:11:38.785308Z",
     "shell.execute_reply": "2025-05-10T01:11:38.784391Z"
    },
    "papermill": {
     "duration": 0.353038,
     "end_time": "2025-05-10T01:11:38.786646",
     "exception": false,
     "start_time": "2025-05-10T01:11:38.433608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize models and optimizers\n",
    "G = Generator(z_dim).to(device)\n",
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4d84a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:38.799563Z",
     "iopub.status.busy": "2025-05-10T01:11:38.798744Z",
     "iopub.status.idle": "2025-05-10T01:11:38.806288Z",
     "shell.execute_reply": "2025-05-10T01:11:38.805168Z"
    },
    "papermill": {
     "duration": 0.014672,
     "end_time": "2025-05-10T01:11:38.807277",
     "exception": false,
     "start_time": "2025-05-10T01:11:38.792605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65bc898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:11:38.819627Z",
     "iopub.status.busy": "2025-05-10T01:11:38.818869Z",
     "iopub.status.idle": "2025-05-10T01:47:08.366254Z",
     "shell.execute_reply": "2025-05-10T01:47:08.365758Z"
    },
    "papermill": {
     "duration": 2129.5597,
     "end_time": "2025-05-10T01:47:08.371870",
     "exception": false,
     "start_time": "2025-05-10T01:11:38.812170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    G.train()\n",
    "    D.train()\n",
    "    running_loss_G = 0.0\n",
    "    running_loss_D = 0.0\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    for real_imgs, _ in tqdm(train_loader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, device=device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z).detach()\n",
    "        D_real = D(real_imgs)\n",
    "        D_fake = D(fake_imgs)\n",
    "        loss_D = criterion(D_real, real_labels) + criterion(D_fake, fake_labels)\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "        D_fake = D(fake_imgs)\n",
    "        loss_G = criterion(D_fake, real_labels)\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        running_loss_D += loss_D.item()\n",
    "        running_loss_G += loss_G.item()\n",
    "\n",
    "    avg_train_loss_G = running_loss_G / len(train_loader)\n",
    "    avg_train_loss_D = running_loss_D / len(train_loader)\n",
    "    train_losses.append((avg_train_loss_D, avg_train_loss_G))\n",
    "\n",
    "    # Validation step\n",
    "    G.eval()\n",
    "    D.eval()\n",
    "    val_loss_D = 0.0\n",
    "    val_loss_G = 0.0\n",
    "    with torch.no_grad():\n",
    "        for real_imgs, _ in val_loader:\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_labels = torch.ones(batch_size, device=device)\n",
    "            fake_labels = torch.zeros(batch_size, device=device)\n",
    "\n",
    "            # Discriminator validation\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "            fake_imgs = G(z)\n",
    "            D_real = D(real_imgs)\n",
    "            D_fake = D(fake_imgs)\n",
    "            loss_D = criterion(D_real, real_labels) + criterion(D_fake, fake_labels)\n",
    "\n",
    "            # Generator validation\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "            fake_imgs = G(z)\n",
    "            D_fake = D(fake_imgs)\n",
    "            loss_G = criterion(D_fake, real_labels)\n",
    "\n",
    "            val_loss_D += loss_D.item()\n",
    "            val_loss_G += loss_G.item()\n",
    "\n",
    "    avg_val_loss_D = val_loss_D / len(val_loader)\n",
    "    avg_val_loss_G = val_loss_G / len(val_loader)\n",
    "    val_losses.append((avg_val_loss_D, avg_val_loss_G))\n",
    "\n",
    "    print(f\"Train Loss D: {avg_train_loss_D:.4f}, G: {avg_train_loss_G:.4f} | Val Loss D: {avg_val_loss_D:.4f}, G: {avg_val_loss_G:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if  avg_val_loss_G < best_val_loss:\n",
    "        best_val_loss = avg_val_loss_G\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(G.state_dict(), \"gan_outputs/best_generator.pth\")\n",
    "        torch.save(D.state_dict(), \"gan_outputs/best_discriminator.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Save sample images\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = G(fixed_noise).detach().cpu()\n",
    "        grid = make_grid(fake_imgs, padding=2, normalize=True)\n",
    "        save_image(grid, f\"gan_outputs/epoch_{epoch+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ee406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:47:08.409221Z",
     "iopub.status.busy": "2025-05-10T01:47:08.409059Z",
     "iopub.status.idle": "2025-05-10T01:47:08.659674Z",
     "shell.execute_reply": "2025-05-10T01:47:08.658930Z"
    },
    "papermill": {
     "duration": 0.270469,
     "end_time": "2025-05-10T01:47:08.660353",
     "exception": false,
     "start_time": "2025-05-10T01:47:08.389884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "train_loss_D, train_loss_G = zip(*train_losses)\n",
    "val_loss_D, val_loss_G = zip(*val_losses)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_D, label=\"Train Loss D\")\n",
    "plt.plot(train_loss_G, label=\"Train Loss G\")\n",
    "plt.plot(val_loss_D, label=\"Val Loss D\")\n",
    "plt.plot(val_loss_G, label=\"Val Loss G\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Losses\")\n",
    "plt.savefig(\"gan_outputs/loss_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714e694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T01:47:08.726248Z",
     "iopub.status.busy": "2025-05-10T01:47:08.726115Z",
     "iopub.status.idle": "2025-05-10T01:47:08.818079Z",
     "shell.execute_reply": "2025-05-10T01:47:08.817769Z"
    },
    "papermill": {
     "duration": 0.11812,
     "end_time": "2025-05-10T01:47:08.818812",
     "exception": false,
     "start_time": "2025-05-10T01:47:08.700692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load generator\n",
    "G = Generator(z_dim=100).to(device)\n",
    "G.load_state_dict(torch.load(\"gan_outputs/best_generator.pth\"))\n",
    "G.eval()\n",
    "\n",
    "# Output directory\n",
    "os.makedirs(\"gan_outputs/generated\", exist_ok=True)\n",
    "\n",
    "# Generate 10,000 images in batches\n",
    "n_images = 10000\n",
    "batch_size = 100\n",
    "z_dim = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, n_images, batch_size)):\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "\n",
    "        for j, img in enumerate(fake_imgs):\n",
    "            idx = i + j\n",
    "            save_path = f\"gan_outputs/generated/img_{idx:05d}.png\"\n",
    "            save_image(img, save_path, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real images\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=\"celebA\", transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Load generator\n",
    "G = Generator(z_dim=100).to(device)\n",
    "G.load_state_dict(torch.load(\"gan_outputs/best_generator.pth\"))\n",
    "G.eval()\n",
    "\n",
    "# Function to find z that reconstructs a real image\n",
    "def invert_image(real_img, generator, z_dim=100, steps=300, lr=0.1):\n",
    "    z = torch.randn(1, z_dim, 1, 1, device=device, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([z], lr=lr)\n",
    "    target = real_img.to(device)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        generated = generator(z)\n",
    "        loss = F.mse_loss(generated, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return z.detach()\n",
    "\n",
    "# Invert latent codes from real images\n",
    "real_z_vectors = []\n",
    "os.makedirs(\"gan_outputs/improved_generated\", exist_ok=True)\n",
    "\n",
    "for i, (img, _) in enumerate(loader):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    z_optimized = invert_image(img, G, z_dim=100)\n",
    "    real_z_vectors.append(z_optimized)\n",
    "\n",
    "# Interpolate between consecutive real-image latent vectors\n",
    "steps = 10\n",
    "with torch.no_grad():\n",
    "    for i in range(len(real_z_vectors) - 1):\n",
    "        z1 = real_z_vectors[i]\n",
    "        z2 = real_z_vectors[i + 1]\n",
    "        for j, alpha in enumerate(torch.linspace(0, 1, steps)):\n",
    "            z_interp = torch.lerp(z1, z2, alpha)\n",
    "            img = G(z_interp).squeeze(0)\n",
    "            save_path = f\"gan_outputs/improved_generated/pair_{i:02d}_step_{j:02d}.png\"\n",
    "            save_image(img, save_path, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2165.805288,
   "end_time": "2025-05-10T01:47:09.770373",
   "environment_variables": {},
   "exception": null,
   "input_path": "celebA-GAN.ipynb",
   "output_path": "scripts/celebA-GAN-20250509_211103.ipynb",
   "parameters": {},
   "start_time": "2025-05-10T01:11:03.965085",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
