{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60825822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1dff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for the CelebA dataset using .csv files.\n",
    "    Supports image-only access or conditioning via attributes, landmarks, and bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the directory containing the dataset CSVs and image folder.\n",
    "        split (str): One of {'train', 'valid', 'test', 'all'}.\n",
    "        target_types (list): Subset of ['attr', 'landmarks', 'bbox'].\n",
    "        transform (callable): Optional transform to apply to images.\n",
    "        target_transform (callable): Optional transform to apply to targets.\n",
    "        return_dict (bool): If True, returns targets as a dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        split='all',\n",
    "        target_types=None,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        return_dict=False\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_folder = os.path.join(root_dir, \"img_align_celeba\")\n",
    "        self.split = split\n",
    "        self.target_types = target_types if target_types else []\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.return_dict = return_dict\n",
    "\n",
    "        if not os.path.isdir(self.img_folder):\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.img_folder}\")\n",
    "\n",
    "        # Load image split\n",
    "        split_path = os.path.join(root_dir, \"list_eval_partition.csv\")\n",
    "        partition_df = pd.read_csv(split_path)\n",
    "        if split != 'all':\n",
    "            split_map = {'train': 0, 'valid': 1, 'test': 2}\n",
    "            partition_df = partition_df[partition_df['partition'] == split_map[split]]\n",
    "        self.image_files = partition_df['image_id'].tolist()\n",
    "\n",
    "        # Load all metadata into dictionaries\n",
    "        self.target_data = {}\n",
    "        for target_type in self.target_types:\n",
    "            df = self._load_csv(target_type)\n",
    "            # Create a subset of the dataframe containing only images in our split\n",
    "            df_subset = df[df['image_id'].isin(self.image_files)]\n",
    "            self.target_data[target_type] = {}\n",
    "            \n",
    "            # Process each row individually to avoid type conversion issues\n",
    "            for _, row in df_subset.iterrows():\n",
    "                img_id = row['image_id']\n",
    "                try:\n",
    "                    self.target_data[target_type][img_id] = self._convert_to_tensor(row, target_type)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_id} for {target_type}: {e}\")\n",
    "                    # Skip problematic entries rather than failing completely\n",
    "\n",
    "    def _convert_to_tensor(self, row, target_type):\n",
    "        \"\"\"Convert row data to appropriate tensor based on target type\"\"\"\n",
    "        if target_type == 'attr':\n",
    "            # Attributes are -1/1 in the csv, convert to 0/1\n",
    "            # Access values as a list rather than numpy array to handle mixed types\n",
    "            values = [int(row[col]) for col in row.index[1:]]  # Skip image_id\n",
    "            values = [(v + 1) // 2 for v in values]  # Convert -1/1 to 0/1\n",
    "            return torch.tensor(values, dtype=torch.float32)\n",
    "        elif target_type == 'landmarks':\n",
    "            # For landmarks, ensure we're getting numeric values\n",
    "            values = [float(row[col]) for col in row.index[1:]]\n",
    "            return torch.tensor(values, dtype=torch.float32)\n",
    "        else:  # 'bbox'\n",
    "            # For bbox, convert to float values\n",
    "            values = [float(row[col]) for col in row.index[1:]]\n",
    "            return torch.tensor(values, dtype=torch.float32)\n",
    "\n",
    "    def _load_csv(self, target_type):\n",
    "        \"\"\"Load the appropriate CSV file based on target type\"\"\"\n",
    "        if target_type == 'attr':\n",
    "            path = os.path.join(self.root_dir, 'list_attr_celeba.csv')\n",
    "            # Convert string values to integers when loading\n",
    "            df = pd.read_csv(path)\n",
    "            # Convert all non-image_id columns to numeric\n",
    "            for col in df.columns[1:]:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        elif target_type == 'landmarks':\n",
    "            path = os.path.join(self.root_dir, 'list_landmarks_align_celeba.csv')\n",
    "            df = pd.read_csv(path)\n",
    "            # Convert all non-image_id columns to numeric\n",
    "            for col in df.columns[1:]:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        elif target_type == 'bbox':\n",
    "            path = os.path.join(self.root_dir, 'list_bbox_celeba.csv')\n",
    "            df = pd.read_csv(path)\n",
    "            # Convert all non-image_id columns to numeric\n",
    "            for col in df.columns[1:]:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_folder, img_id)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if not self.target_types:\n",
    "            return image\n",
    "\n",
    "        targets = []\n",
    "        for t in self.target_types:\n",
    "            # Get target data for this image, empty tensor if not found\n",
    "            value = self.target_data.get(t, {}).get(img_id, torch.tensor([]))\n",
    "            targets.append(value)\n",
    "\n",
    "        if self.target_transform:\n",
    "            targets = self.target_transform(targets)\n",
    "\n",
    "        if self.return_dict:\n",
    "            return image, {k: v for k, v in zip(self.target_types, targets)}\n",
    "        elif len(targets) == 1:\n",
    "            return image, targets[0]\n",
    "        return image, tuple(targets)\n",
    "\n",
    "\n",
    "def get_celeba_dataloader(\n",
    "    root_dir='.',\n",
    "    split='train',\n",
    "    batch_size=32,\n",
    "    image_size=64,\n",
    "    target_types=None,\n",
    "    transform=None,\n",
    "    target_transform=None,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    return_dict=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the CelebA dataset.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the dataset root\n",
    "        split (str): One of {'train', 'valid', 'test', 'all'}\n",
    "        batch_size (int): Batch size\n",
    "        image_size (int): Size to resize images to (square)\n",
    "        target_types (list): Subset of ['attr', 'landmarks', 'bbox']\n",
    "        transform (callable): Transform to apply to images\n",
    "        target_transform (callable): Transform to apply to targets\n",
    "        num_workers (int): Number of worker processes for data loading\n",
    "        shuffle (bool): Whether to shuffle the data\n",
    "        return_dict (bool): If True, targets are returned as dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader for the CelebA dataset\n",
    "    \"\"\"\n",
    "    # Standard transformation pipeline for images\n",
    "    if split == 'train' and transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)), # Resize to square\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
    "        ])\n",
    "    \n",
    "    elif transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),  \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "        ])    \n",
    "\n",
    "    dataset = CelebADataset(\n",
    "        root_dir=root_dir,\n",
    "        split=split,\n",
    "        target_types=target_types,\n",
    "        transform=transform,\n",
    "        return_dict=return_dict\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f5d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_embed_dim=None):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        \n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "        if time_embed_dim is not None:\n",
    "            self.time_mlp = nn.Linear(time_embed_dim, out_channels)\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.act(self.norm1(self.conv1(x)))\n",
    "        \n",
    "        if self.time_embed_dim is not None and time_emb is not None:\n",
    "            time_emb = self.act(self.time_mlp(time_emb))\n",
    "            h = h + time_emb.unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "        h = self.act(self.norm2(self.conv2(h)))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a689686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49802c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_dim=128, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.init_conv = nn.Conv2d(in_channels, features[0], kernel_size=3, padding=1)\n",
    "        \n",
    "        # Encoder pathway\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(features) - 1):\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                Block(features[i], features[i], time_dim),\n",
    "                Block(features[i], features[i+1], time_dim),\n",
    "                nn.MaxPool2d(kernel_size=2)\n",
    "            ]))\n",
    "            \n",
    "        # Middle blocks\n",
    "        self.middle = nn.ModuleList([\n",
    "            Block(features[-1], features[-1], time_dim),\n",
    "            Block(features[-1], features[-1], time_dim)\n",
    "        ])\n",
    "        \n",
    "        # Decoder pathway\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in reversed(range(len(features) - 1)):\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                Block(features[i+1], features[i+1], time_dim),\n",
    "                Block(features[i+1], features[i], time_dim),\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "            ]))\n",
    "            \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Sequential(\n",
    "            Block(features[0], features[0], time_dim),\n",
    "            nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t = self.time_mlp(t)\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = self.init_conv(x)\n",
    "        \n",
    "        # Store residual connections\n",
    "        residuals = []\n",
    "        \n",
    "        # Encoder\n",
    "        for down_block1, down_block2, downsample in self.downs:\n",
    "            x = down_block1(x, t)\n",
    "            x = down_block2(x, t)\n",
    "            residuals.append(x)\n",
    "            x = downsample(x)\n",
    "            \n",
    "        # Middle\n",
    "        x = self.middle[0](x, t)\n",
    "        x = self.middle[1](x, t)\n",
    "        \n",
    "        # Decoder\n",
    "        for up_block1, up_block2, upsample in self.ups:\n",
    "            x = up_block1(x, t)\n",
    "            x = up_block2(x, t)\n",
    "            x = upsample(x)\n",
    "            residual = residuals.pop()\n",
    "            \n",
    "            # Handle potential size mismatch\n",
    "            if x.shape != residual.shape:\n",
    "                x = F.interpolate(x, size=residual.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "                \n",
    "            x = x + residual  # Skip connection\n",
    "            \n",
    "        # Final convolution\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "260c1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    \"\"\"Extract coefficients at specified timesteps t and reshape to match x_shape\"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09063b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self, device, num_timesteps=500, beta_start=1e-4, beta_end=0.02):\n",
    "        self.device = device\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Define beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        # Define alphas\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Pre-calculate diffusion parameters\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1. / self.alphas)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"Forward diffusion process\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def p_losses(self, denoise_model, x_start, t, noise=None):\n",
    "        \"\"\"Calculate training loss for the denoising model\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "        # Add noise to the input image according to timestep t\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        \n",
    "        # Predict the noise using the model\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t, t_index):\n",
    "        \"\"\"Sample from the model at timestep t\"\"\"\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas, t, x.shape)\n",
    "        \n",
    "        # Equation 11 in the paper\n",
    "        # Use model to predict the mean\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "        \n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            # Algorithm 2 in the paper\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        \"\"\"Generate samples from the model using the sampling loop\"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        b = shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        \n",
    "        # Progressively denoise the image\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling timesteps', total=self.num_timesteps):\n",
    "            # Create a batch of the same timestep\n",
    "            t = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            # Sample from p(x_{t-1} | x_t)\n",
    "            img = self.p_sample(model, img, t, i)\n",
    "            \n",
    "        # Samples are in [-1, 1] range\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, batch_size=16, channels=3, img_size=64):\n",
    "        \"\"\"Simple interface for sampling from the model\"\"\"\n",
    "        return self.p_sample_loop(model, shape=(batch_size, channels, img_size, img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f067786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(diffusion, model, dataloader, optimizer, num_epochs=10, device=\"cpu\"):\n",
    "    \"\"\"Train the diffusion model\"\"\"\n",
    "    os.makedirs(\"diffusion_outputs\", exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i, batch in progress_bar:\n",
    "            # print(len(batch))\n",
    "            # print(type(batch))\n",
    "            # print(batch.shape)\n",
    "            images = batch\n",
    "            \n",
    "            \n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, diffusion.num_timesteps, (images.shape[0],), device=device).long()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = diffusion.p_losses(model, images, t)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss/(i+1))\n",
    "            \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(checkpoint, f\"diffusion_outputs/checkpoint_epoch_{epoch+1}.pt\")\n",
    "            \n",
    "        # Generate and save samples\n",
    "        if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            sample_images = diffusion.sample(model, batch_size=4, img_size=64)\n",
    "            # Rescale from [-1, 1] to [0, 1]\n",
    "            sample_images = (sample_images + 1) / 2\n",
    "            save_image(sample_images, f\"diffusion_outputs/samples_epoch_{epoch+1}.png\", nrow=2)\n",
    "            \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"diffusion_outputs/final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 128, 128])\n",
      "attr shape: torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = get_celeba_dataloader(\n",
    "    root_dir='./celeba',\n",
    "    split='train',\n",
    "    batch_size=32,\n",
    "    image_size=128,\n",
    "    target_types=['attr'],\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "for images, targets in test_dataloader:\n",
    "        print(f\"Image batch shape: {images.shape}\")\n",
    "        if isinstance(targets, dict):\n",
    "            for target_type, target_tensor in targets.items():\n",
    "                print(f\"{target_type} shape: {target_tensor.shape}\")\n",
    "        else:\n",
    "            print(f\"Target shape: {targets.shape}\")\n",
    "        break\n",
    "\n",
    "# Show the first 5 images in the batch 'images'\n",
    "from matplotlib import pyplot as plt\n",
    "imgs = images[:5]\n",
    "\n",
    "for img in imgs:\n",
    "    img = img.permute(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
    "    img = (img + 1) / 2  # Rescale to [0, 1]\n",
    "    img = img.numpy()\n",
    "    # img = np.clip(img, 0, 1)  # Ensure values are in [0, 1]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()  # Move plt.show() outside the loop to display all images at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c145e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, diffusion, output_dir, total_images=10000, batch_size=16, device=\"cpu\"):\n",
    "    \"\"\"Generate a large number of images efficiently\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = (total_images + batch_size - 1) // batch_size\n",
    "    image_count = 0\n",
    "    \n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Generating batches\"):\n",
    "        # For the last batch, adjust batch size if needed\n",
    "        current_batch_size = min(batch_size, total_images - image_count)\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        with torch.no_grad():\n",
    "            batch_images = diffusion.sample(model, batch_size=current_batch_size, img_size=64)\n",
    "            # Rescale from [-1, 1] to [0, 1]\n",
    "            batch_images = (batch_images + 1) / 2\n",
    "            \n",
    "            # Save as a grid for inspection\n",
    "            if batch_idx % 10 == 0:\n",
    "                save_image(batch_images, os.path.join(output_dir, f\"grid_{batch_idx}.png\"), nrow=4)\n",
    "            \n",
    "            # Save individual images\n",
    "            for i in range(current_batch_size):\n",
    "                filename = os.path.join(output_dir, f\"img_{image_count:05d}.png\")\n",
    "                save_image(batch_images[i], filename)\n",
    "                image_count += 1\n",
    "                \n",
    "                if image_count >= total_images:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e4ae88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def main():\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(\"diffusion_outputs\", exist_ok=True)\n",
    "    os.makedirs(\"diffusion_outputs/generated\", exist_ok=True)\n",
    "    \n",
    "    # Dataset preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(160),\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    # try:\n",
    "    #     image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "    #     dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "    #     dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    #     print(f\"Loaded dataset with {len(dataset)} images\")\n",
    "    # except Exception as e:\n",
    "    #     # print(f\"Error loading dataset: {e}\")\n",
    "    #     logging.error(f\"Error loading dataset: {e}\", exc_info=True, stack_info=True)\n",
    "    #     print(\"Make sure the CelebA dataset is available at the specified path\")\n",
    "    #     return\n",
    "    \n",
    "    dataloader = get_celeba_dataloader(\n",
    "        root_dir='./celeba',\n",
    "        split='train',\n",
    "        batch_size=32,\n",
    "        image_size=64,\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleUNet(in_channels=3, out_channels=3, time_dim=128).to(device)\n",
    "    \n",
    "    # Initialize diffusion process (using fewer timesteps for a more practical demonstration)\n",
    "    diffusion = DiffusionModel(device=device, num_timesteps=500)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_diffusion(diffusion, model, dataloader, optimizer, num_epochs=10, device=device)\n",
    "    \n",
    "    # Generate images\n",
    "    print(\"Generating 10,000 images...\")\n",
    "    generate_images(model, diffusion, \"diffusion_outputs/generated\", total_images=10000, batch_size=16, device=device)\n",
    "    \n",
    "    print(\"Done! 10,000 images saved in: diffusion_outputs/generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f926a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/5087 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Generate images\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating 10,000 images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[0;34m(diffusion, model, dataloader, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(batch))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 16\u001b[0m images, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     19\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e34ecdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sawyer/01_classwork/genVision-celebA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d58e171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_img_dir = Path(\"celebA/celeba/img_align_celeba\")\n",
    "test_img_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17347359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
