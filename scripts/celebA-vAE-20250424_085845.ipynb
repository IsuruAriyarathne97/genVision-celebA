{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befe7dc5",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb563499",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d060246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T12:58:46.783757Z",
     "iopub.status.busy": "2025-04-24T12:58:46.783175Z",
     "iopub.status.idle": "2025-04-24T12:59:01.445504Z",
     "shell.execute_reply": "2025-04-24T12:59:01.444280Z"
    },
    "papermill": {
     "duration": 14.669784,
     "end_time": "2025-04-24T12:59:01.446168",
     "exception": true,
     "start_time": "2025-04-24T12:58:46.776384",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/iahewababarand/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scr/iahewababarand/TMPDIR/ipykernel_2986464/4255752010.py:88: UserWarning: Using a target size (torch.Size([64, 3, 64, 64])) that is different to the input size (torch.Size([64, 3, 32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m optimizer.zero_grad()\n\u001b[32m    107\u001b[39m recon_batch, mu, logvar = model(data)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m loss = \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m loss.backward()\n\u001b[32m    110\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mloss_function\u001b[39m\u001b[34m(recon_x, x, mu, logvar)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss_function\u001b[39m(recon_x, x, mu, logvar):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     recon_loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     KLD = -\u001b[32m0.5\u001b[39m * torch.sum(\u001b[32m1\u001b[39m + logvar - mu.pow(\u001b[32m2\u001b[39m) - logvar.exp())\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m recon_loss + KLD\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sciclone/data10/iahewababarand/.venv/lib64/python3.11/site-packages/torch/nn/functional.py:3884\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3882\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sciclone/data10/iahewababarand/.venv/lib64/python3.11/site-packages/torch/functional.py:76\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset path\n",
    "data_root = \"celebA/celeba\"\n",
    "image_dir = os.path.join(data_root, \"img_align_celeba\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),  # Changed to 64x64\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "half_len = len(dataset) // 2\n",
    "subset = Subset(dataset, range(half_len))\n",
    "\n",
    "dataloader = DataLoader(subset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Convolutional VAE for 64x64 images\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),   # 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),# 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),# 2x2\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512 * 2 * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 2 * 2, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, 512 * 2 * 2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),     # 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_mu(x), self.fc_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z)\n",
    "        x = x.view(-1, 512, 2, 2)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + KLD\n",
    "\n",
    "# Initialize model, optimizer, fixed noise\n",
    "latent_dim = 128\n",
    "model = ConvVAE(latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "fixed_z = torch.randn(64, latent_dim).to(device)\n",
    "os.makedirs(\"vae_outputs\", exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save generated samples after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.decode(fixed_z).cpu()\n",
    "        generated = (generated + 1) / 2\n",
    "        save_image(generated, f\"vae_outputs/epoch_{epoch+1}.png\", nrow=8)\n",
    "\n",
    "# Final generation example\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, latent_dim).to(device)\n",
    "    sample = model.decode(z).cpu()\n",
    "    sample = (sample + 1) / 2\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(make_grid(sample, nrow=8).permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate 10,000 images at 64x64\n",
    "output_dir = \"vae_outputs/generated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "total_images = 10000\n",
    "batch_size = 100\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating 10,000 images...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, total_images, batch_size):\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "        samples = (samples + 1) / 2\n",
    "        for j in range(samples.size(0)):\n",
    "            idx = i + j\n",
    "            filename = os.path.join(output_dir, f\"img_{idx:05d}.png\")\n",
    "            save_image(samples[j], filename)\n",
    "\n",
    "print(\"Done! 10,000 images saved in:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.380533,
   "end_time": "2025-04-24T12:59:04.118377",
   "environment_variables": {},
   "exception": true,
   "input_path": "celebA-vAE.ipynb",
   "output_path": "scripts/celebA-vAE-20250424_085845.ipynb",
   "parameters": {},
   "start_time": "2025-04-24T12:58:45.737844",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}