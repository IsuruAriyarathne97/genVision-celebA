{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e2ec82",
   "metadata": {
    "papermill": {
     "duration": 0.009716,
     "end_time": "2025-05-09T15:12:32.964294",
     "exception": false,
     "start_time": "2025-05-09T15:12:32.954578",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d075782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:32.977363Z",
     "iopub.status.busy": "2025-05-09T15:12:32.976976Z",
     "iopub.status.idle": "2025-05-09T15:12:36.281500Z",
     "shell.execute_reply": "2025-05-09T15:12:36.280956Z"
    },
    "papermill": {
     "duration": 3.312024,
     "end_time": "2025-05-09T15:12:36.282706",
     "exception": false,
     "start_time": "2025-05-09T15:12:32.970682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49a0137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:36.290453Z",
     "iopub.status.busy": "2025-05-09T15:12:36.289416Z",
     "iopub.status.idle": "2025-05-09T15:12:36.294123Z",
     "shell.execute_reply": "2025-05-09T15:12:36.293723Z"
    },
    "papermill": {
     "duration": 0.009218,
     "end_time": "2025-05-09T15:12:36.295055",
     "exception": false,
     "start_time": "2025-05-09T15:12:36.285837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd40c8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:36.302025Z",
     "iopub.status.busy": "2025-05-09T15:12:36.301332Z",
     "iopub.status.idle": "2025-05-09T15:12:45.568828Z",
     "shell.execute_reply": "2025-05-09T15:12:45.568223Z"
    },
    "papermill": {
     "duration": 9.272022,
     "end_time": "2025-05-09T15:12:45.569914",
     "exception": false,
     "start_time": "2025-05-09T15:12:36.297892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"test\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8eefab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:45.597799Z",
     "iopub.status.busy": "2025-05-09T15:12:45.596759Z",
     "iopub.status.idle": "2025-05-09T15:12:45.605838Z",
     "shell.execute_reply": "2025-05-09T15:12:45.605380Z"
    },
    "papermill": {
     "duration": 0.015362,
     "end_time": "2025-05-09T15:12:45.606899",
     "exception": false,
     "start_time": "2025-05-09T15:12:45.591537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Latent vector z_dim → 1024 filters → 4x4\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 1024x4x4 → 512x8x8\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512x8x8 → 256x16x16\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256x16x16 → 128x32x32\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128x32x32 → img_channels x64x64\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.output(self.main_module(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427dd99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:45.614954Z",
     "iopub.status.busy": "2025-05-09T15:12:45.614233Z",
     "iopub.status.idle": "2025-05-09T15:12:45.620602Z",
     "shell.execute_reply": "2025-05-09T15:12:45.620193Z"
    },
    "papermill": {
     "duration": 0.01135,
     "end_time": "2025-05-09T15:12:45.621604",
     "exception": false,
     "start_time": "2025-05-09T15:12:45.610254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Filters: [64, 128, 256, 512]\n",
    "        # Input_dim = img_channels (3 for RGB)\n",
    "        # Output_dim = 1 (real/fake score)\n",
    "        \n",
    "        self.main_module = nn.Sequential(\n",
    "            # Input: img_channels x64x64\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64x32x32\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 128x16x16\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 256x8x8\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 512x4x4 → output 1x1\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1246626a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:45.628918Z",
     "iopub.status.busy": "2025-05-09T15:12:45.628228Z",
     "iopub.status.idle": "2025-05-09T15:12:45.633788Z",
     "shell.execute_reply": "2025-05-09T15:12:45.633372Z"
    },
    "papermill": {
     "duration": 0.010125,
     "end_time": "2025-05-09T15:12:45.634739",
     "exception": false,
     "start_time": "2025-05-09T15:12:45.624614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient Penalty function\n",
    "def gradient_penalty(D, real, fake, device):\n",
    "    # Get the batch size (number of samples in the batch)\n",
    "    batch_size = real.size(0)\n",
    "    # Generate random interpolation coefficients epsilon for each sample\n",
    "    # Shape: [batch_size, 1, 1, 1] so it can broadcast across all image dimensions (C, H, W)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    # Interpolate between real and fake images:\n",
    "    # interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # This creates points along the lines between real and fake samples\n",
    "    interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # Tell PyTorch to track gradients for interpolated samples (needed to compute gradient penalty)\n",
    "    interpolated.requires_grad_(True)\n",
    "    # Pass interpolated samples through the discriminator (critic) to get scores\n",
    "    interp_logits = D(interpolated)\n",
    "    # Compute gradients of the critic output w.r.t. the interpolated samples\n",
    "    # grad() returns a tuple → take the first element\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interp_logits,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(interp_logits, device=device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # Flatten gradients for each sample into vectors (combine channel, height, width into one dimension)\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    # Compute L2 norm (Euclidean norm) of gradients for each sample\n",
    "    # This measures how large the gradients are for each interpolated image\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    #Compute gradient penalty:\n",
    "    # Penalize the squared difference from 1 for each sample's gradient norm\n",
    "    # Then take the average across the batch\n",
    "    gp = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4b017d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:45.641982Z",
     "iopub.status.busy": "2025-05-09T15:12:45.641297Z",
     "iopub.status.idle": "2025-05-09T15:12:45.757917Z",
     "shell.execute_reply": "2025-05-09T15:12:45.757451Z"
    },
    "papermill": {
     "duration": 0.121284,
     "end_time": "2025-05-09T15:12:45.758960",
     "exception": false,
     "start_time": "2025-05-09T15:12:45.637676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define latent vector size (input noise dimension for generator)\n",
    "z_dim = 100\n",
    "# Set learning rate for both generator and discriminator optimizers\n",
    "lr = 2e-4\n",
    "# Number of epochs to train\n",
    "n_epochs = 5\n",
    "# Batch size for training\n",
    "batch_size = 64\n",
    "# Initialize the Generator model and move it to the selected device \n",
    "G = Generator(z_dim).to(device)\n",
    "# Initialize the Discriminator (critic) model and move it to the selected device\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Adam optimizer for the generator and discriminator\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare a fixed random noise vector to generate consistent sample images across epochs \n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7aa65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:12:45.767564Z",
     "iopub.status.busy": "2025-05-09T15:12:45.766825Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-05-09T15:12:45.762738",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 0/3166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 1/3166 [00:15<13:27:07, 15.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 2/3166 [00:28<12:33:23, 14.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 3/3166 [00:42<12:19:22, 14.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 4/3166 [00:56<12:09:59, 13.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 5/3166 [01:09<12:02:48, 13.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 6/3166 [01:23<11:59:44, 13.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 7/3166 [01:36<11:56:52, 13.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 8/3166 [01:50<11:55:45, 13.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 9/3166 [02:03<11:54:57, 13.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 10/3166 [02:17<11:54:38, 13.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 11/3166 [02:31<11:54:08, 13.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 12/3166 [02:44<11:53:29, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 13/3166 [02:58<11:53:43, 13.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 14/3166 [03:11<11:55:54, 13.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 15/3166 [03:25<11:53:57, 13.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 16/3166 [03:38<11:53:12, 13.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 17/3166 [03:52<11:54:30, 13.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 18/3166 [04:06<11:54:51, 13.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 19/3166 [04:19<11:52:52, 13.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 20/3166 [04:33<11:51:32, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 21/3166 [04:46<11:50:02, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 22/3166 [05:00<11:50:54, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 23/3166 [05:14<11:52:40, 13.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 24/3166 [05:27<11:52:14, 13.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 25/3166 [05:41<11:50:27, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 26/3166 [05:54<11:49:09, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 27/3166 [06:08<11:47:43, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 28/3166 [06:21<11:46:14, 13.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 29/3166 [06:35<11:46:00, 13.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 30/3166 [06:48<11:46:44, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 31/3166 [07:02<11:46:27, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 32/3166 [07:15<11:45:56, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 33/3166 [07:29<11:45:47, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 34/3166 [07:42<11:45:32, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 35/3166 [07:56<11:45:14, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 36/3166 [08:09<11:44:47, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 37/3166 [08:23<11:44:31, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 38/3166 [08:36<11:45:37, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 39/3166 [08:50<11:45:42, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 40/3166 [09:03<11:45:22, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 41/3166 [09:17<11:44:37, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 42/3166 [09:30<11:43:51, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 43/3166 [09:44<11:44:10, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 44/3166 [09:58<11:44:12, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 45/3166 [10:11<11:44:03, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 46/3166 [10:25<11:44:35, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|▏         | 47/3166 [10:38<11:44:35, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 48/3166 [10:52<11:46:06, 13.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 49/3166 [11:05<11:45:11, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 50/3166 [11:19<11:43:49, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 51/3166 [11:33<11:43:48, 13.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 52/3166 [11:46<11:44:10, 13.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 53/3166 [12:00<11:43:06, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 54/3166 [12:13<11:42:35, 13.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 55/3166 [12:27<11:42:49, 13.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 56/3166 [12:40<11:41:52, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 57/3166 [12:54<11:40:55, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 58/3166 [13:07<11:39:31, 13.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 59/3166 [13:21<11:40:50, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 60/3166 [13:34<11:40:33, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 61/3166 [13:48<11:40:46, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 62/3166 [14:01<11:39:31, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 63/3166 [14:15<11:39:28, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 64/3166 [14:28<11:38:28, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 65/3166 [14:42<11:36:57, 13.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 66/3166 [14:55<11:33:42, 13.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 67/3166 [15:09<11:32:41, 13.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 68/3166 [15:22<11:32:24, 13.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   2%|▏         | 69/3166 [15:35<11:31:42, 13.40s/it]"
     ]
    }
   ],
   "source": [
    "# Start looping over all epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize epoch-level loss trackers\n",
    "    g_loss_epoch = 0.0\n",
    "    d_loss_epoch = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Loop over the dataloader that provides batches of real images\n",
    "    for real_imgs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        real_imgs = real_imgs.to(device)  # Move real images to GPU/CPU\n",
    "        batch_size = real_imgs.size(0)    # Get actual batch size\n",
    "\n",
    "        # Set hyperparameters once \n",
    "        d_iterations = 5\n",
    "        lambda_gp = 10\n",
    "\n",
    "        # Train the Discriminator multiple times\n",
    "        for _ in range(d_iterations):\n",
    "            # Sample random noise\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "            # Get discrimnator outputs for real and fake images\n",
    "            output_real = D(real_imgs)\n",
    "            output_fake = D(fake_imgs.detach())  # detach so gradients don’t flow into generator\n",
    "\n",
    "            # Compute Wasserstein loss for discriminator\n",
    "            loss_D = -(output_real.mean() - output_fake.mean())\n",
    "\n",
    "            # Compute gradient penalty\n",
    "            gp = gradient_penalty(D, real_imgs, fake_imgs.detach(), device)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            loss_D += lambda_gp * gp\n",
    "\n",
    "            # Update Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "        output_fake = D(fake_imgs)\n",
    "        loss_G = -output_fake.mean()  # Generator tries to make D(fake) large\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        g_loss_epoch += loss_G.item()\n",
    "        d_loss_epoch += loss_D.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Generate and save sample images using fixed noise\n",
    "    with torch.no_grad():\n",
    "        samples = G(fixed_noise)\n",
    "        save_image(samples, f\"wgan_outputs/epoch_{epoch+1:03d}.png\", normalize=True)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]  Loss_D: {d_loss_epoch/num_batches:.4f}  Loss_G: {g_loss_epoch/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74e271",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate 10,000 Images\n",
    "os.makedirs(\"wgan_outputs/generated\", exist_ok=True)\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, 10000, 64), desc=\"Generating final images\"):\n",
    "        z = torch.randn(64, z_dim, 1, 1, device=device)\n",
    "        gen_imgs = G(z)\n",
    "        for j in range(gen_imgs.size(0)):\n",
    "            save_image(gen_imgs[j], f\"wgan_outputs/generated/{i + j:05d}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d694f30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('gen-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "celebA-WGAN.ipynb",
   "output_path": "scripts/celebA-WGAN-20250509_111231.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T15:12:31.903496",
   "version": "2.6.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "20be29547590aa1112ef8e25a79f1428227ffeb142cb4839bcd469c0aee95255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}