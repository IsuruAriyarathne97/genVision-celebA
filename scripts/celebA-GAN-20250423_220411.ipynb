{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21de21fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T02:04:12.706195Z",
     "iopub.status.busy": "2025-04-24T02:04:12.705611Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-24T02:04:12.698799",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/iahewababarand/genVision-VAE/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/iahewababarand/genVision-VAE/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/sciclone/data10/iahewababarand/genVision-VAE/.venv/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 0/3166 Loss D: -6.3916, loss G: 4.5593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 100/3166 Loss D: -26.6402, loss G: 21.0721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 200/3166 Loss D: -19.2689, loss G: 33.9940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 300/3166 Loss D: -18.1477, loss G: 35.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 400/3166 Loss D: -20.4166, loss G: 28.7506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 500/3166 Loss D: -14.0392, loss G: 37.1196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 600/3166 Loss D: -15.8908, loss G: 39.4243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 700/3166 Loss D: -15.1784, loss G: 47.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 800/3166 Loss D: -18.3646, loss G: 34.2869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 900/3166 Loss D: -14.4248, loss G: 49.5545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1000/3166 Loss D: -17.2217, loss G: 42.7312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1100/3166 Loss D: -15.5086, loss G: 43.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1200/3166 Loss D: -17.2078, loss G: 44.6685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1300/3166 Loss D: -17.3383, loss G: 52.6133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1400/3166 Loss D: -15.2380, loss G: 51.0174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1500/3166 Loss D: -15.6955, loss G: 52.2828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1600/3166 Loss D: -16.0453, loss G: 61.3832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1700/3166 Loss D: -12.6293, loss G: 56.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1800/3166 Loss D: -16.7912, loss G: 50.9496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 1900/3166 Loss D: -13.0867, loss G: 55.7634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2000/3166 Loss D: -13.1293, loss G: 56.8894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2100/3166 Loss D: -14.0013, loss G: 48.4248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2200/3166 Loss D: -14.9455, loss G: 63.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2300/3166 Loss D: -14.7145, loss G: 52.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2400/3166 Loss D: -14.3453, loss G: 58.8153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2500/3166 Loss D: -14.6316, loss G: 48.7392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2600/3166 Loss D: -14.5610, loss G: 69.7032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2700/3166 Loss D: -13.8260, loss G: 64.6036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2800/3166 Loss D: -13.5897, loss G: 52.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 2900/3166 Loss D: -14.3526, loss G: 56.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 3000/3166 Loss D: -13.5358, loss G: 57.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Batch 3100/3166 Loss D: -12.3521, loss G: 59.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Batch 0/3166 Loss D: -11.0298, loss G: 55.2957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Batch 100/3166 Loss D: -14.7308, loss G: 63.5315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Batch 200/3166 Loss D: -14.1501, loss G: 57.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Batch 300/3166 Loss D: -14.6483, loss G: 48.4475\n"
     ]
    }
   ],
   "source": [
    "# WGAN-GP for CelebA 64x64 Face Generation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# 1. Models: Generator & Critic\n",
    "# -----------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "# -----------------------\n",
    "# 2. Gradient Penalty\n",
    "# -----------------------\n",
    "def gradient_penalty(critic, real, fake, device=\"cuda\"):\n",
    "    batch_size, c, h, w = real.shape\n",
    "    epsilon = torch.rand((batch_size, 1, 1, 1), device=device).repeat(1, c, h, w)\n",
    "    interpolated = real * epsilon + fake * (1 - epsilon)\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = critic(interpolated)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gp = ((gradient.norm(2, dim=1) - 1)**2).mean()\n",
    "    return gp\n",
    "\n",
    "# -----------------------\n",
    "# 3. Training Setup\n",
    "# -----------------------\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"wgan_outputs\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "z_dim = 128\n",
    "gen = Generator(z_dim).to(device)\n",
    "critic = Critic().to(device)\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=1e-4, betas=(0.0, 0.9))\n",
    "\n",
    "# -----------------------\n",
    "# 4. Training Loop\n",
    "# -----------------------\n",
    "critic_iters = 5\n",
    "epochs = 3\n",
    "lambda_gp = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.size(0)\n",
    "\n",
    "        # Train critic\n",
    "        for _ in range(critic_iters):\n",
    "            z = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
    "            fake = gen(z).detach()\n",
    "            critic_real = critic(real)\n",
    "            critic_fake = critic(fake)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) + lambda_gp * gp\n",
    "\n",
    "            opt_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            opt_critic.step()\n",
    "\n",
    "        # Train generator\n",
    "        z = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
    "        fake = gen(z)\n",
    "        loss_gen = -torch.mean(critic(fake))\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Batch {batch_idx}/{len(dataloader)} \"\n",
    "                  f\"Loss D: {loss_critic.item():.4f}, loss G: {loss_gen.item():.4f}\")\n",
    "\n",
    "    # Save samples\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(64, z_dim, 1, 1).to(device)\n",
    "        samples = gen(z)\n",
    "        samples = (samples + 1) / 2\n",
    "        save_image(samples, f\"wgan_outputs/epoch_{epoch+1}.png\", nrow=8)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Generate 100,000 Images\n",
    "# -----------------------\n",
    "print(\"Generating 100,000 images...\")\n",
    "output_dir = \"wgan_outputs/generated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gen.eval()\n",
    "batch_size = 100\n",
    "for i in range(0, 100000, batch_size):\n",
    "    z = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "    with torch.no_grad():\n",
    "        fake = gen(z).cpu()\n",
    "        fake = (fake + 1) / 2\n",
    "        for j in range(fake.size(0)):\n",
    "            idx = i + j\n",
    "            save_image(fake[j], f\"wgan_generated/generated/image_{idx:05d}.png\")\n",
    "\n",
    "print(\"Done. 100,000 images saved to 'wgan_generated/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "celebA-GAN.ipynb",
   "output_path": "scripts/celebA-GAN-20250423_220411.ipynb",
   "parameters": {},
   "start_time": "2025-04-24T02:04:11.614886",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}