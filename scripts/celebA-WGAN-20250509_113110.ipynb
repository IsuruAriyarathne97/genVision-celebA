{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f559a702",
   "metadata": {
    "papermill": {
     "duration": 0.010985,
     "end_time": "2025-05-09T15:31:12.353834",
     "exception": false,
     "start_time": "2025-05-09T15:31:12.342849",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c260b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:12.371750Z",
     "iopub.status.busy": "2025-05-09T15:31:12.371152Z",
     "iopub.status.idle": "2025-05-09T15:31:18.116727Z",
     "shell.execute_reply": "2025-05-09T15:31:18.115346Z"
    },
    "papermill": {
     "duration": 5.757061,
     "end_time": "2025-05-09T15:31:18.119705",
     "exception": false,
     "start_time": "2025-05-09T15:31:12.362644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c959fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:18.135919Z",
     "iopub.status.busy": "2025-05-09T15:31:18.134391Z",
     "iopub.status.idle": "2025-05-09T15:31:18.141982Z",
     "shell.execute_reply": "2025-05-09T15:31:18.141237Z"
    },
    "papermill": {
     "duration": 0.016341,
     "end_time": "2025-05-09T15:31:18.143645",
     "exception": false,
     "start_time": "2025-05-09T15:31:18.127304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4f99e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:18.154186Z",
     "iopub.status.busy": "2025-05-09T15:31:18.153762Z",
     "iopub.status.idle": "2025-05-09T15:31:47.346736Z",
     "shell.execute_reply": "2025-05-09T15:31:47.346118Z"
    },
    "papermill": {
     "duration": 29.199357,
     "end_time": "2025-05-09T15:31:47.347854",
     "exception": false,
     "start_time": "2025-05-09T15:31:18.148497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"test\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea653f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:47.357696Z",
     "iopub.status.busy": "2025-05-09T15:31:47.356952Z",
     "iopub.status.idle": "2025-05-09T15:31:47.364371Z",
     "shell.execute_reply": "2025-05-09T15:31:47.363957Z"
    },
    "papermill": {
     "duration": 0.013322,
     "end_time": "2025-05-09T15:31:47.365332",
     "exception": false,
     "start_time": "2025-05-09T15:31:47.352010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Latent vector z_dim → 1024 filters → 4x4\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 1024x4x4 → 512x8x8\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512x8x8 → 256x16x16\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256x16x16 → 128x32x32\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128x32x32 → img_channels x64x64\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.output(self.main_module(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec68c975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:47.374853Z",
     "iopub.status.busy": "2025-05-09T15:31:47.374147Z",
     "iopub.status.idle": "2025-05-09T15:31:47.380434Z",
     "shell.execute_reply": "2025-05-09T15:31:47.380017Z"
    },
    "papermill": {
     "duration": 0.011856,
     "end_time": "2025-05-09T15:31:47.381379",
     "exception": false,
     "start_time": "2025-05-09T15:31:47.369523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Filters: [64, 128, 256, 512]\n",
    "        # Input_dim = img_channels (3 for RGB)\n",
    "        # Output_dim = 1 (real/fake score)\n",
    "        \n",
    "        self.main_module = nn.Sequential(\n",
    "            # Input: img_channels x64x64\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64x32x32\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 128x16x16\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 256x8x8\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 512x4x4 → output 1x1\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed51bc5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:47.390548Z",
     "iopub.status.busy": "2025-05-09T15:31:47.389854Z",
     "iopub.status.idle": "2025-05-09T15:31:47.395430Z",
     "shell.execute_reply": "2025-05-09T15:31:47.395019Z"
    },
    "papermill": {
     "duration": 0.011169,
     "end_time": "2025-05-09T15:31:47.396375",
     "exception": false,
     "start_time": "2025-05-09T15:31:47.385206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient Penalty function\n",
    "def gradient_penalty(D, real, fake, device):\n",
    "    # Get the batch size (number of samples in the batch)\n",
    "    batch_size = real.size(0)\n",
    "    # Generate random interpolation coefficients epsilon for each sample\n",
    "    # Shape: [batch_size, 1, 1, 1] so it can broadcast across all image dimensions (C, H, W)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    # Interpolate between real and fake images:\n",
    "    # interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # This creates points along the lines between real and fake samples\n",
    "    interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # Tell PyTorch to track gradients for interpolated samples (needed to compute gradient penalty)\n",
    "    interpolated.requires_grad_(True)\n",
    "    # Pass interpolated samples through the discriminator (critic) to get scores\n",
    "    interp_logits = D(interpolated)\n",
    "    # Compute gradients of the critic output w.r.t. the interpolated samples\n",
    "    # grad() returns a tuple → take the first element\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interp_logits,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(interp_logits, device=device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # Flatten gradients for each sample into vectors (combine channel, height, width into one dimension)\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    # Compute L2 norm (Euclidean norm) of gradients for each sample\n",
    "    # This measures how large the gradients are for each interpolated image\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    #Compute gradient penalty:\n",
    "    # Penalize the squared difference from 1 for each sample's gradient norm\n",
    "    # Then take the average across the batch\n",
    "    gp = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00383cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:47.405627Z",
     "iopub.status.busy": "2025-05-09T15:31:47.404933Z",
     "iopub.status.idle": "2025-05-09T15:31:47.520830Z",
     "shell.execute_reply": "2025-05-09T15:31:47.520424Z"
    },
    "papermill": {
     "duration": 0.121432,
     "end_time": "2025-05-09T15:31:47.521769",
     "exception": false,
     "start_time": "2025-05-09T15:31:47.400337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define latent vector size (input noise dimension for generator)\n",
    "z_dim = 100\n",
    "# Set learning rate for both generator and discriminator optimizers\n",
    "lr = 2e-4\n",
    "# Number of epochs to train\n",
    "n_epochs = 5\n",
    "# Batch size for training\n",
    "batch_size = 64\n",
    "# Initialize the Generator model and move it to the selected device \n",
    "G = Generator(z_dim).to(device)\n",
    "# Initialize the Discriminator (critic) model and move it to the selected device\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Adam optimizer for the generator and discriminator\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare a fixed random noise vector to generate consistent sample images across epochs \n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ba25a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T15:31:47.531953Z",
     "iopub.status.busy": "2025-05-09T15:31:47.531247Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-05-09T15:31:47.526128",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 0/3166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 1/3166 [00:15<13:25:03, 15.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 2/3166 [00:28<12:29:31, 14.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 3/3166 [00:42<12:11:03, 13.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 4/3166 [00:55<12:04:02, 13.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 5/3166 [01:09<12:01:17, 13.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 6/3166 [01:22<11:57:14, 13.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 7/3166 [01:36<11:53:50, 13.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 8/3166 [01:49<11:54:46, 13.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 9/3166 [02:03<11:52:03, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 10/3166 [02:16<11:50:39, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 11/3166 [02:30<11:51:23, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 12/3166 [02:43<11:48:51, 13.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 13/3166 [02:57<11:48:25, 13.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 14/3166 [03:10<11:47:47, 13.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   0%|          | 15/3166 [03:24<11:47:36, 13.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 16/3166 [03:37<11:46:36, 13.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 17/3166 [03:51<11:47:22, 13.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 18/3166 [04:04<11:45:34, 13.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 19/3166 [04:17<11:46:21, 13.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 20/3166 [04:31<11:49:39, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 21/3166 [04:45<11:48:40, 13.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 22/3166 [04:58<11:48:03, 13.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 23/3166 [05:12<11:49:04, 13.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 24/3166 [05:25<11:48:21, 13.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 25/3166 [05:39<11:46:29, 13.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 26/3166 [05:52<11:46:29, 13.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/5:   1%|          | 27/3166 [06:06<11:46:34, 13.51s/it]"
     ]
    }
   ],
   "source": [
    "# Start looping over all epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize epoch-level loss trackers\n",
    "    g_loss_epoch = 0.0\n",
    "    d_loss_epoch = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Loop over the dataloader that provides batches of real images\n",
    "    for real_imgs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        real_imgs = real_imgs.to(device)  # Move real images to GPU/CPU\n",
    "        batch_size = real_imgs.size(0)    # Get actual batch size\n",
    "\n",
    "        # Set hyperparameters once \n",
    "        d_iterations = 5\n",
    "        lambda_gp = 10\n",
    "\n",
    "        # Train the Discriminator multiple times\n",
    "        for _ in range(d_iterations):\n",
    "            # Sample random noise\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "            # Get discrimnator outputs for real and fake images\n",
    "            output_real = D(real_imgs)\n",
    "            output_fake = D(fake_imgs.detach())  # detach so gradients don’t flow into generator\n",
    "\n",
    "            # Compute Wasserstein loss for discriminator\n",
    "            loss_D = -(output_real.mean() - output_fake.mean())\n",
    "\n",
    "            # Compute gradient penalty\n",
    "            gp = gradient_penalty(D, real_imgs, fake_imgs.detach(), device)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            loss_D += lambda_gp * gp\n",
    "\n",
    "            # Update Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "        output_fake = D(fake_imgs)\n",
    "        loss_G = -output_fake.mean()  # Generator tries to make D(fake) large\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        g_loss_epoch += loss_G.item()\n",
    "        d_loss_epoch += loss_D.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Generate and save sample images using fixed noise\n",
    "    with torch.no_grad():\n",
    "        samples = G(fixed_noise)\n",
    "        save_image(samples, f\"wgan_outputs/epoch_{epoch+1:03d}.png\", normalize=True)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]  Loss_D: {d_loss_epoch/num_batches:.4f}  Loss_G: {g_loss_epoch/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88a78b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate 10,000 Images\n",
    "os.makedirs(\"wgan_outputs/generated\", exist_ok=True)\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, 10000, 64), desc=\"Generating final images\"):\n",
    "        z = torch.randn(64, z_dim, 1, 1, device=device)\n",
    "        gen_imgs = G(z)\n",
    "        for j in range(gen_imgs.size(0)):\n",
    "            save_image(gen_imgs[j], f\"wgan_outputs/generated/{i + j:05d}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99007b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('gen-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "celebA-WGAN.ipynb",
   "output_path": "scripts/celebA-WGAN-20250509_113110.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T15:31:10.375967",
   "version": "2.6.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "20be29547590aa1112ef8e25a79f1428227ffeb142cb4839bcd469c0aee95255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}