{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA/gen-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sciclone/data10/jrhee01/genVision-celebA\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "\n",
    "image_dir = \"celebA/celeba/img_align_celeba\"\n",
    "os.makedirs(\"test\", exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, z_dim=100, img_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Latent vector z_dim → 1024 filters → 4x4\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 1024x4x4 → 512x8x8\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512x8x8 → 256x16x16\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256x16x16 → 128x32x32\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128x32x32 → img_channels x64x64\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.output(self.main_module(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Filters: [64, 128, 256, 512]\n",
    "        # Input_dim = img_channels (3 for RGB)\n",
    "        # Output_dim = 1 (real/fake score)\n",
    "        \n",
    "        self.main_module = nn.Sequential(\n",
    "            # Input: img_channels x64x64\n",
    "            nn.Conv2d(in_channels=img_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 64x32x32\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 128x16x16\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 256x8x8\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 512x4x4 → output 1x1\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Penalty function\n",
    "def gradient_penalty(D, real, fake, device):\n",
    "    # Get the batch size (number of samples in the batch)\n",
    "    batch_size = real.size(0)\n",
    "    # Generate random interpolation coefficients epsilon for each sample\n",
    "    # Shape: [batch_size, 1, 1, 1] so it can broadcast across all image dimensions (C, H, W)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    # Interpolate between real and fake images:\n",
    "    # interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # This creates points along the lines between real and fake samples\n",
    "    interpolated = epsilon * real + (1 - epsilon) * fake\n",
    "    # Tell PyTorch to track gradients for interpolated samples (needed to compute gradient penalty)\n",
    "    interpolated.requires_grad_(True)\n",
    "    # Pass interpolated samples through the discriminator (critic) to get scores\n",
    "    interp_logits = D(interpolated)\n",
    "    # Compute gradients of the critic output w.r.t. the interpolated samples\n",
    "    # grad() returns a tuple → take the first element\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interp_logits,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(interp_logits, device=device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # Flatten gradients for each sample into vectors (combine channel, height, width into one dimension)\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    # Compute L2 norm (Euclidean norm) of gradients for each sample\n",
    "    # This measures how large the gradients are for each interpolated image\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    #Compute gradient penalty:\n",
    "    # Penalize the squared difference from 1 for each sample's gradient norm\n",
    "    # Then take the average across the batch\n",
    "    gp = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define latent vector size (input noise dimension for generator)\n",
    "z_dim = 100\n",
    "# Set learning rate for both generator and discriminator optimizers\n",
    "lr = 2e-4\n",
    "# Number of epochs to train\n",
    "n_epochs = 5\n",
    "# Batch size for training\n",
    "batch_size = 64\n",
    "# Initialize the Generator model and move it to the selected device \n",
    "G = Generator(z_dim).to(device)\n",
    "# Initialize the Discriminator (critic) model and move it to the selected device\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Adam optimizer for the generator and discriminator\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Prepare a fixed random noise vector to generate consistent sample images across epochs \n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start looping over all epochs\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize epoch-level loss trackers\n",
    "    g_loss_epoch = 0.0\n",
    "    d_loss_epoch = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Loop over the dataloader that provides batches of real images\n",
    "    for real_imgs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        real_imgs = real_imgs.to(device)  # Move real images to GPU/CPU\n",
    "        batch_size = real_imgs.size(0)    # Get actual batch size\n",
    "\n",
    "        # Set hyperparameters once \n",
    "        d_iterations = 5\n",
    "        lambda_gp = 10\n",
    "\n",
    "        # Train the Discriminator multiple times\n",
    "        for _ in range(d_iterations):\n",
    "            # Sample random noise\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "            # Get discrimnator outputs for real and fake images\n",
    "            output_real = D(real_imgs)\n",
    "            output_fake = D(fake_imgs.detach())  # detach so gradients don’t flow into generator\n",
    "\n",
    "            # Compute Wasserstein loss for discriminator\n",
    "            loss_D = -(output_real.mean() - output_fake.mean())\n",
    "\n",
    "            # Compute gradient penalty\n",
    "            gp = gradient_penalty(D, real_imgs, fake_imgs.detach(), device)\n",
    "\n",
    "            # Total discriminator loss\n",
    "            loss_D += lambda_gp * gp\n",
    "\n",
    "            # Update Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "        fake_imgs = G(z)\n",
    "        output_fake = D(fake_imgs)\n",
    "        loss_G = -output_fake.mean()  # Generator tries to make D(fake) large\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        g_loss_epoch += loss_G.item()\n",
    "        d_loss_epoch += loss_D.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Generate and save sample images using fixed noise\n",
    "    with torch.no_grad():\n",
    "        samples = G(fixed_noise)\n",
    "        save_image(samples, f\"wgan_outputs/epoch_{epoch+1:03d}.png\", normalize=True)\n",
    "\n",
    "    # Print average losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]  Loss_D: {d_loss_epoch/num_batches:.4f}  Loss_G: {g_loss_epoch/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10,000 Images\n",
    "os.makedirs(\"wgan_outputs/generated\", exist_ok=True)\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, 10000, 64), desc=\"Generating final images\"):\n",
    "        z = torch.randn(64, z_dim, 1, 1, device=device)\n",
    "        gen_imgs = G(z)\n",
    "        for j in range(gen_imgs.size(0)):\n",
    "            save_image(gen_imgs[j], f\"wgan_outputs/generated/{i + j:05d}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('gen-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20be29547590aa1112ef8e25a79f1428227ffeb142cb4839bcd469c0aee95255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
